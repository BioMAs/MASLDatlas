# Guide de gestion des gros datasets MASLDatlas

## ProblÃ¨me : Dataset "Fibrotic Integrated Cross Species-002" (9.2 GB)

Ce dataset intÃ©grÃ© de 9.2 GB pose des problÃ¨mes de performance :
- â±ï¸ Chargement trÃ¨s lent (30+ minutes)
- ğŸ’¾ Consommation mÃ©moire importante (>16 GB RAM)
- ğŸš« Blocage de l'interface utilisateur

## Solutions mises en place

### 1. Configuration temporaire allÃ©gÃ©e âœ…
**Fichier** : `config/datasets_config_temp.json`
- Exclut temporairement le gros dataset
- Permet l'utilisation normale des autres datasets
- Solution immÃ©diate pour dÃ©veloppement/tests

### 2. Interface de sÃ©lection de taille ğŸš§
**Fichier** : `app.R` (lignes 150-170)
- SÃ©lecteur de taille pour datasets volumineux
- Options : 5k, 10k, 20k cellules ou dataset complet
- Avertissements pour dataset complet

### 3. Gestion d'erreurs avancÃ©e âœ…
**Fichier** : `app.R` (lignes 620-680)
- Progress bars pour chargement
- Messages d'erreur informatifs
- Fallback vers versions optimisÃ©es

### 4. Scripts d'optimisation ğŸš§
**Fichiers** : 
- `scripts/dataset-management/optimize_large_dataset.py`
- `scripts/dataset-management/optimize_large_dataset.R`
- `scripts/dataset-management/create_optimized_datasets.sh`

## Prochaines Ã©tapes recommandÃ©es

### Option 1 : Installation de scanpy (RecommandÃ©e)
```bash
# Installer scanpy dans l'environnement Python
pip install scanpy pandas numpy

# ExÃ©cuter l'optimisation
cd /Users/tdarde/Documents/GitHub/MASLDatlas
./scripts/dataset-management/create_optimized_datasets.sh
```

### Option 2 : HÃ©bergement externe
- DÃ©placer le dataset vers un serveur de donnÃ©es
- ImplÃ©menter un chargement Ã  la demande
- API de sous-Ã©chantillonnage cÃ´tÃ© serveur

### Option 3 : PrÃ©-traitement externe
- CrÃ©er manuellement des versions Ã©chantillonnÃ©es
- Utiliser des outils comme Seurat ou scanpy en local
- Placer les fichiers optimisÃ©s dans `datasets_optimized/`

## Structure de fichiers optimisÃ©s

```
datasets_optimized/
â”œâ”€â”€ Fibrotic Integrated Cross Species-002_sub5k.h5ad     (~100-200 MB)
â”œâ”€â”€ Fibrotic Integrated Cross Species-002_sub10k.h5ad    (~200-400 MB)
â”œâ”€â”€ Fibrotic Integrated Cross Species-002_sub20k.h5ad    (~400-800 MB)
â””â”€â”€ Fibrotic Integrated Cross Species-002_metadata.h5ad  (~10-50 MB)
```

## Configuration de production

Une fois les datasets optimisÃ©s crÃ©Ã©s :

1. **Revenir Ã  la configuration complÃ¨te** :
```r
# Dans app.R, ligne 61
datasets_config <- jsonlite::fromJSON("config/datasets_config.json")
```

2. **Mettre Ã  jour datasets_config.json** :
```json
{
  "Integrated": {
    "Datasets": [
      "Fibrotic Integrated Cross Species-002_sub5k",
      "Fibrotic Integrated Cross Species-002_sub10k", 
      "Fibrotic Integrated Cross Species-002_sub20k",
      "Fibrotic Integrated Cross Species-002"
    ]
  }
}
```

## Surveillance des performances

- **MÃ©moire** : Surveiller l'usage RAM avec `htop`
- **Temps de chargement** : Logs dans l'application Shiny
- **ExpÃ©rience utilisateur** : Tests avec diffÃ©rentes tailles

## Contact

Pour questions ou assistance avec l'optimisation des datasets :
- VÃ©rifier les logs d'erreur Docker
- Consulter la documentation scanpy
- Adapter les scripts selon l'environnement local
